Overview

This project implements the Transformer architecture from scratch in pure Python/PyTorch, inspired by
Andrej Karpathyâ€™s nanogpt-lecture.

The project reproduces a character level Transformer-based language model, trained on a text dataset(tiny shakespeare).

The goal was to deeply understand attention, multi-head attention, positional encodings and residual connections.